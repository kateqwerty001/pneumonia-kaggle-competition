{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pneumonia2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchmetrics.classification import AUROC\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "VAL_SPLIT = 0.2  # 80% train, 20% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Mapping: {'NORMAL': 0, 'PNEUMONIA': 1}\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.ImageFolder(root=\"pneumonia/train\", transform=transform)\n",
    "\n",
    "labels = np.array([sample[1] for sample in dataset.samples])  # Extract labels from dataset\n",
    "\n",
    "# Train-validation split with stratification\n",
    "train_indices, val_indices = train_test_split(\n",
    "    np.arange(len(labels)), test_size=VAL_SPLIT, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Create subset datasets\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Print class mapping\n",
    "print(f\"Class Mapping: {dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PneumoniaModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(PneumoniaModel, self).__init__()\n",
    "        self.backbone = models.convnext_base(weights=\"IMAGENET1K_V1\")  # Use \"convnext_small\" if needed\n",
    "\n",
    "\n",
    "        for param in self.backbone.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        in_features = self.backbone.classifier[2].in_features\n",
    "        self.backbone.classifier[2] = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.backbone(x)  \n",
    "        probs = torch.sigmoid(logits) \n",
    "        return probs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PneumoniaModel(num_classes=2).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.backbone.classifier.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1) \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1) \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE).float() \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).squeeze(1) \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()  \n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        val_auroc = evaluate_auroc(model, val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val AUROC: {val_auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_metric = AUROC(task=\"binary\")  # Use \"multiclass\" for >2 classes\n",
    "\n",
    "def evaluate_auroc(model, val_loader):\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images).squeeze(1)\n",
    "            probs = torch.sigmoid(outputs) \n",
    "\n",
    "            probs_list.append(probs.cpu())\n",
    "            labels_list.append(labels.cpu())\n",
    "\n",
    "    probs_list = torch.cat(probs_list)\n",
    "    labels_list = torch.cat(labels_list)\n",
    "\n",
    "    auroc_score = auroc_metric(probs_list, labels_list)\n",
    "    return auroc_score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5560, Train Acc: 74.28%, Val AUROC: 0.9684\n",
      "Epoch 2/10, Loss: 0.4953, Train Acc: 74.28%, Val AUROC: 0.9725\n",
      "Epoch 3/10, Loss: 0.4734, Train Acc: 74.28%, Val AUROC: 0.9761\n",
      "Epoch 4/10, Loss: 0.4629, Train Acc: 74.28%, Val AUROC: 0.9786\n",
      "Epoch 5/10, Loss: 0.4565, Train Acc: 74.28%, Val AUROC: 0.9807\n",
      "Epoch 6/10, Loss: 0.4522, Train Acc: 74.28%, Val AUROC: 0.9821\n",
      "Epoch 7/10, Loss: 0.4489, Train Acc: 74.28%, Val AUROC: 0.9834\n",
      "Epoch 8/10, Loss: 0.4463, Train Acc: 74.28%, Val AUROC: 0.9844\n",
      "Epoch 9/10, Loss: 0.4443, Train Acc: 74.28%, Val AUROC: 0.9853\n",
      "Epoch 10/10, Loss: 0.4425, Train Acc: 74.28%, Val AUROC: 0.9862\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 624 test images.\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\") \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_path \n",
    "\n",
    "test_dir = \"pneumonia/test\"\n",
    "test_dataset = TestDataset(test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Loaded {len(test_dataset)} test images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved\n"
     ]
    }
   ],
   "source": [
    "def predict_test_set(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, image_paths in test_loader: \n",
    "            images = images.to(DEVICE)\n",
    "\n",
    "            outputs = model(images) \n",
    "            probs = torch.sigmoid(outputs).squeeze(1) \n",
    "\n",
    "            filenames = [os.path.basename(path) for path in image_paths]\n",
    "            predictions.extend(list(zip(filenames, probs.cpu().numpy())))\n",
    "    predictions.sort(key=lambda x: int(os.path.splitext(x[0])[0])) \n",
    "    return predictions\n",
    "\n",
    "test_preds = predict_test_set(model, test_loader)\n",
    "\n",
    "df = pd.DataFrame(test_preds, columns=[\"Id\", \"Category\"])\n",
    "df.to_csv(\"convNetXt_IMAGENET1K_V1__test_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Test predictions saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"convNetXt_IMAGENET1K_V1.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pneumonia2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
